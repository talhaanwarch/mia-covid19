{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp-mixer",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talhaanwarch/mia-covid19/blob/main/mlp_mixer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcvlecBwfVG-",
        "outputId": "40eb2300-268c-4408-8ba2-a007b735648c"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-cd7c5b43-2b3f-15d6-ed73-855d9569cc5a)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYCrx53mdfOV",
        "outputId": "063ff173-2b6c-4b88-aa0d-d93922a52d1c"
      },
      "source": [
        "#connect drive with colab notebook/\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSJFX50WeAuA"
      },
      "source": [
        "!cp /content/drive/MyDrive/covid/train.zip /content/\n",
        "!cp /content/drive/MyDrive/covid/val.zip /content/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHOYdivIgOuo"
      },
      "source": [
        "%%capture \n",
        "!unzip /content/train.zip\n",
        "!rm /content/train.zip\n",
        "!mv /content/resized /content/train\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INRl2NyagnOd"
      },
      "source": [
        "%%capture \n",
        "!unzip /content/val.zip -d val\n",
        "!rm /content/val.zip\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQZnjjHeUD0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import cv2\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vaPmBTzetmj"
      },
      "source": [
        "# #load data, data loaded is in format of list of list containing folder and image paths\n",
        "# train_pos_path=[glob(folder+'*.jpg') for folder in glob('train/covid/*/')]\n",
        "# train_neg_path=[glob(folder+'*.jpg') for folder in glob('train/non-covid/*/')]\n",
        "# len(train_pos_path),len(train_neg_path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnrJiX2xe1oD"
      },
      "source": [
        "# #create labels for list of list images\n",
        "# train_pos_label=[[0]*len(i)  for i in train_pos_path]\n",
        "# train_neg_label=[[1]*len(i)  for i in train_neg_path]\n",
        "# len(train_pos_label),len(train_neg_label)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0lfrq4we1q0"
      },
      "source": [
        "# #combine data\n",
        "# train_path=train_pos_path+train_neg_path\n",
        "# train_label=train_pos_label+train_neg_label\n",
        "# len(train_path),len(train_label)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InFE1HS_e1tZ"
      },
      "source": [
        "# #plit data \n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train_path,val_path,train_label,val_label=train_test_split(train_path,train_label,test_size=0.2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dThoXTzye1wb"
      },
      "source": [
        "# #flatten list\n",
        "# train_path = [item for sublist in train_path for item in sublist]\n",
        "# val_path = [item for sublist in val_path for item in sublist]\n",
        "# train_label = [item for sublist in train_label for item in sublist]\n",
        "# val_label = [item for sublist in val_label for item in sublist]\n",
        "# len(train_path),len(train_label),len(val_path),len(val_label)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93AMfV08e1zM"
      },
      "source": [
        "# train_df=pd.DataFrame(zip(train_path,train_label),columns=['img','label'])\n",
        "# train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "# train_df.to_csv('train_df.csv',index=False)\n",
        "# train_df.head()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa-9mmr4e11s"
      },
      "source": [
        "# val_df=pd.DataFrame(zip(val_path,val_label),columns=['img','label'])\n",
        "# val_df = val_df.sample(frac=1).reset_index(drop=True)\n",
        "# val_df.to_csv('val_df.csv',index=False)\n",
        "# val_df.head()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSQSy5-Ee14V"
      },
      "source": [
        "# #calculate mean and standard deviation of random images\n",
        "# mean,std=[],[]\n",
        "# for i in train_df.img[0:100]:\n",
        "#     img=cv2.imread(i,0)/255.0\n",
        "#     mean.append(np.mean(img)),std.append(np.std(img))\n",
        "    \n",
        "# print('mean',np.mean(mean))\n",
        "# print('std',np.mean(std))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "ws-hYMlXe17T",
        "outputId": "7363682e-eb66-422f-f765-079d31220802"
      },
      "source": [
        "train_df=pd.read_csv('/content/drive/MyDrive/covid/train_df.csv')\n",
        "val_df=pd.read_csv('/content/drive/MyDrive/covid/val_df.csv')\n",
        "train_df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/train/non-covid/ct_scan_307/376.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/train/covid/ct_scan_206/2.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/train/covid/ct_scan_95/266.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/train/covid/ct_scan_579/4.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/train/covid/ct_scan_185/7.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          image  label\n",
              "0  /content/train/non-covid/ct_scan_307/376.jpg      0\n",
              "1        /content/train/covid/ct_scan_206/2.jpg      1\n",
              "2       /content/train/covid/ct_scan_95/266.jpg      1\n",
              "3        /content/train/covid/ct_scan_579/4.jpg      1\n",
              "4        /content/train/covid/ct_scan_185/7.jpg      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jo72jzHe8Ie"
      },
      "source": [
        "import torch \n",
        "from torchvision import transforms \n",
        "import torchvision.models as models\n",
        "import torch.nn as nn"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-XwPy3le_Rr"
      },
      "source": [
        "aug=transforms.Compose([\n",
        "                        #transforms.Resize((224,224)),\n",
        "                        #transforms.Grayscale(),\n",
        "                        transforms.RandomHorizontalFlip(p=0.5),\n",
        "                        transforms.RandomVerticalFlip(p=0.5),\n",
        "                        transforms.RandomRotation(degrees=5),\n",
        "                        transforms.CenterCrop(224),\n",
        "                        transforms.RandomPerspective(0.1,0.2),\n",
        "                        transforms.RandomAffine(5,(0.05,0.05),(0.05,0.05)),\n",
        "                        \n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize([0.5, ], [0.5,]),\n",
        "\n",
        "                        ])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqwG1w1OfAZU"
      },
      "source": [
        "from PIL import Image\n",
        "class dfloader(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self,img_id,img_label,transform=None):\n",
        "        self.img_id = img_id    \n",
        "        self.img_label=img_label\n",
        "        self.transform=transform\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        if type(index) == torch.Tensor:\n",
        "          index = index.item()\n",
        "        img_ind=self.img_id[index]\n",
        "        label_ind=self.img_label[index]\n",
        "\n",
        "        img = Image.open(img_ind).convert('L')\n",
        "      \n",
        "        if self.transform:\n",
        "           img=self.transform(img) \n",
        "  \n",
        "        return img ,label_ind\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_id)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2GGevtewH6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4723557-d404-4853-88e1-d2e7eb92b979"
      },
      "source": [
        "!git clone https://github.com/jaketae/mlp-mixer.git\n",
        "import sys\n",
        "sys.path.append('/content/mlp-mixer')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mlp-mixer'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 47 (delta 18), reused 24 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwS7_1AFz50P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924ca370-ced3-41ad-a711-7fb1eacd6b5e"
      },
      "source": [
        "from mlp_mixer import MLPMixer\n",
        "model = MLPMixer(image_size=224,in_channels=1,num_layers=4,num_classes=1)\n",
        "img = torch.randn(1, 1, 224, 224)\n",
        "pred = model(img) \n",
        "print(pred)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0286]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI-79-OifAef"
      },
      "source": [
        "from sklearn.metrics import classification_report,f1_score\n",
        "from time import time\n",
        "\n",
        "def get_accuracy(y_true, y_pred):\n",
        "    assert y_true.ndim == 1 and y_true.size() == y_pred.size()\n",
        "    y_pred = y_pred > 0.5\n",
        "    return (y_true == y_pred).sum().item() / y_true.size(0)\n",
        "\n",
        "def fit_train(loader):\n",
        "    loss_sum=0\n",
        "    acc_sum=0\n",
        "    scaler = torch.cuda.amp.GradScaler() \n",
        "    for batch in loader:\n",
        "        img,label=batch\n",
        "        with torch.cuda.amp.autocast(): \n",
        "          img,label = img.to(device),label.to(device)\n",
        "          out=model(img)\n",
        "          loss=criterion(out.view(-1),label.float())\n",
        "        opt.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        #loss.backward()\n",
        "        scaler.step(opt)\n",
        "        #opt.step()\n",
        "        scaler.update()\n",
        "        loss_sum+=loss.item()\n",
        "        acc_sum += get_accuracy(label,out.view(-1))\n",
        "    return loss_sum,acc_sum\n",
        "def fit_val(loader):\n",
        "    loss_sum=0\n",
        "    acc_sum=0\n",
        "    for batch in loader:\n",
        "        img,label=batch\n",
        "        img,label = img.to(device),label.to(device)\n",
        "        out=model(img)\n",
        "        loss=criterion(out.view(-1),label.float())\n",
        "        loss_sum+=loss.item()\n",
        "        acc_sum += get_accuracy(label,out.view(-1))\n",
        "    return loss_sum,acc_sum\n",
        "\n",
        "def fit_test(loader):\n",
        "  test_pred=[]\n",
        "  for img,_ in loader:\n",
        "    test_pred.append(model(img.to(device)))\n",
        "  test_pred=torch.cat(test_pred,dim=0)\n",
        "  y_pred=test_pred.cpu().numpy()\n",
        "  return y_pred\n",
        "\n",
        "\n",
        "def fit(model,train_loader,val_loader,test_loader=None,epoch=10,scheduler_step=None,verbose=None):\n",
        "  \n",
        "  train_loss_plt=[]\n",
        "  val_loss_plt=[]\n",
        "  train_acc_plt=[]\n",
        "  val_acc_plt=[]\n",
        "  for ep in range(epoch):\n",
        "    start=time()\n",
        "    #start training loop\n",
        "    train_loss,train_acc=fit_train(train_loader)\n",
        "    #start validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss,val_acc=fit_val(val_loader)\n",
        "    end=np.round((time()-start)/60,2) #time in minute\n",
        "    model.train()\n",
        "\n",
        "    #calculate print and append the results for plotting purpose\n",
        "    val_avg_loss=np.round(val_loss/len(val_loader),2)#val loss of all batches of one epoch\n",
        "    train_avg_loss=np.round(train_loss/len(train_loader),2)# train loss of all batches of one epoch\n",
        "    train_avg_acc=np.round(train_acc/len(train_loader),2)#train acc of all batches of one epoch\n",
        "    val_avg_acc=np.round(val_acc/len(val_loader),2)#val acc of all batches of one epoch\n",
        "    if scheduler_step:\n",
        "      scheduler.step(val_avg_loss)\n",
        "    if verbose:\n",
        "      print('Epoch {}, time {} min , train acc  {}, train loss {} , val acc is {}, loss is {}, learning rate is {} '.format\n",
        "            (ep,end,train_avg_acc,train_avg_loss,val_avg_acc,val_avg_loss,opt.param_groups[0]['lr']))\n",
        "    train_loss_plt.append(train_avg_loss)  #append loss of training data  \n",
        "    val_loss_plt.append(val_avg_loss)     #append loss of validation data\n",
        "    train_acc_plt.append(train_avg_acc)  #append acc of training data  \n",
        "    val_acc_plt.append(val_avg_acc)     #append acc of validation data\n",
        " \n",
        " #test phase\n",
        "  if test_loader:\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "         y_pred=fit_test(test_loader)\n",
        "  \n",
        "  return [train_loss_plt,val_loss_plt,train_acc_plt,val_acc_plt]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWdQno1kfAhb"
      },
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "train=dfloader(train_df.image.values,train_df.label.values,transform=aug)\n",
        "val=dfloader(val_df.image.values,val_df.label.values,transform=aug)\n",
        "train_loader = DataLoader(train,shuffle=True,num_workers=0,batch_size=512)\n",
        "val_loader = DataLoader(val,shuffle=True,num_workers=0,batch_size=512)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvzWi4q1fAjs"
      },
      "source": [
        "device=torch.device('cuda')\n",
        "model=model.to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opt=torch.optim.AdamW(params=model.parameters(),lr=0.001)\n",
        "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(opt,patience=2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N8YLePIfFRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ee2024-f52c-4101-c668-d4fc7aae6fbb"
      },
      "source": [
        "res=fit(model,train_loader,val_loader,epoch=10,scheduler_step=True,verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, time 17.23 min , train acc  0.55, train loss 0.69 , val acc is 0.51, loss is 0.69, learning rate is 0.001 \n",
            "Epoch 1, time 16.23 min , train acc  0.55, train loss 0.69 , val acc is 0.51, loss is 0.69, learning rate is 0.001 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgIku0QBi4tR"
      },
      "source": [
        "state = {\n",
        "        'epoch': 10,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': opt.state_dict(),\n",
        "}\n",
        "savepath='/content/drive/MyDrive/covid/mlp.pt'\n",
        "torch.save(state,savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZZJs-zn_Z7t"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/covid/mlp1.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba9LtLKonkLp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}